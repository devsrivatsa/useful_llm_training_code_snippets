from transformers import AutoTokenizer
import datasets

dataset = dataset.load_dataset("paraquet", data_files="path", split="train") 
tokenizer = AutoTokenizer.from_pretrained("model_name_or_path", use_fast=True)

------------------------------------------------------------------------------------

def tokenization(example):
    # Tokenize
    tokens = tokenizer.tokenize(example["text"])

    # Convert tokens to ids
    token_ids = tokenizer.convert_tokens_to_ids(tokens)

    # Add <bos>, <eos> tokens to the front and back of tokens_ids 
    # bos: begin of sequence, eos: end of sequence
    token_ids = [
        tokenizer.bos_token_id] \
        + token_ids \
        + [tokenizer.eos_token_id
    ]
    example["input_ids"] = token_ids

    # We will be using this column to count the total number of tokens 
    # in the final dataset
    example["num_tokens"] = len(token_ids)
    return example

dataset = dataset.map(tokenization, load_from_cache_file=False)

--------------------------------------------------------------------------------------

#Concatenate input_ids for all examples into a single list:
import numpy as np

input_ids = np.concatenate(dataset["input_ids"])

#this is max context length of the model
max_seq_length = 32

#Discard extra tokens from end of the list so number of tokens is exactly divisible by max_seq_length
total_length = len(input_ids) - len(input_ids) % max_seq_length
input_ids = input_ids[:total_length]

#reshape dataset
input_ids_reshaped = input_ids.reshape(-1, max_seq_length).astype(np.int32)

#Convert to Hugging Face dataset
input_ids_list = input_ids_reshaped.tolist()
packaged_pretrain_dataset = datasets.Dataset.from_dict(
    {"input_ids": input_ids_list}
)

#save
packaged_pretrain_dataset.to_parquet("./data/packaged_pretrain_dataset.parquet")
